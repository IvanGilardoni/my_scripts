#!/bin/bash

# split by author: all the e-books of a certain author as a single job
# an array containing the authors

cd txt

get_authors()
{
	local IFS='$\n' # because of spaces in the names
	authors=($(printf "%s\n" *txt | sed 's/__.*//' | awk '!x[$0]++')) # remove the second part of the filename: the name of the book
	declare -p authors 
}

get_authors ## but authors are repeated, so delete duplications: sort and uniq are slow, we used awk

for a in "${authors[@]}"
do
	cat "$a".txt | ## all the e-books of this author and count words
		tr A-Z a-z |
		tr -cs a-z '\n'
		awk -v author="$a$" '{x[$0]=0} END{printf "%5d\n", length(x), a(x)}' # how long is the dictionary
	
done # | sort -nr ## sort at the end

how can we parallelize this? there is no dependence between one author and the other
inser compute_dict

compute_dict()
{
	cat "$a".txt | ## all the e-books of this author and count words
		tr A-Z a-z |
		tr -cs a-z '\n'
		awk -v author="$a$" '{x[$0]=0} END{printf "%5d\n", length(x), a(x)}'
}
		
for a in "${authors[@]}"
do
	compute_dict "$a" &
done

wait

## 12 seconds instead of 45s, elapsed CPU 691% instead of 110%
## without & after "$a" 131% CPU because there is a pipe: with a pipe each program runs on a different process, achieving 31% of power

./find_biggest_dict.sh | sort -nr 
